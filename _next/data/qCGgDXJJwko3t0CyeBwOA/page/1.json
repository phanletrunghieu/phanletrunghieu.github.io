{"pageProps":{"categories":["ReactJS","K8s","Docker","GI/CD","Tip","Gitlab","Microservice"],"allBlogs":[{"frontmatter":{"author":"Hieu Phan","date":"2021-03-01T07:00:00.000Z","image":"/images/10-deploy-multi-region-applications-with-k8s/cover.jpg","title":"Deploy multi-region applications with k8s","readDuration":"30 min","categories":["K8s"]},"markdownBody":"\n## 1. Run 2 K8S clusters locally for practicing\n\nCreate 2 clusters with 1 master for each cluster. Image clusters put in different regions.\n\nSee [this post](/post/9-run-multi-node-k8s-cluster-locally).\n\n## 2. Solution\n\nIn my case, I use MariaDB with `replication` architecture.\n\n![Solution](/images/10-deploy-multi-region-applications-with-k8s/1-solution.jpg)\n\nI use `bitnami/mariadb` 9.3.4 chart.\n\nI need run mariadb with primary & secondary in different nodes, but `bitnami/mariadb` allow run 1 primary and many secondary nodes. So I need run some customization.\n\n## 3. Customize `bitnami/mariadb` chart\n\n3.1. Add primary.enabled to `values.yaml`\n\n```yaml\nprimary:\n  enabled: true\n```\n\n3.2. Add condition `.Values.primary.enabled` to all file in `templates/primary`\n\n3.3. Add master's information to secondary in `values.yaml`.\n\n```yaml\nsecondary:\n  master:\n    host:\n    port:\n```\n\n3.4. Update env in `templates/secondary/statefulset.yaml`\n\n```yaml\nenv:\n  - name: MARIADB_MASTER_HOST\n    value: {{ default (include \"mariadb.primary.fullname\" .) .Values.secondary.master.host }}\n  - name: MARIADB_MASTER_PORT_NUMBER\n    value: {{ default .Values.primary.service.port .Values.secondary.master.port | quote }}\n```\n\nSee my final custom chart on [Github](https://github.com/phanletrunghieu/multi-region-k8s)\n\n## 4. Deploy\n\nSwitch `kubectl` to cluster 1 and run\n\n```yaml\nhelm -n app  upgrade --install app . --create-namespace -f values.region-1.yaml\n```\n\nSwitch `kubectl` to cluster 2 and run\n```yaml\nhelm -n app  upgrade --install app . --create-namespace -f values.region-2.yaml\n```","slug":"10-deploy-multi-region-applications-with-k8s"},{"frontmatter":{"author":"Hieu Phan","date":"2021-02-25T07:00:00.000Z","image":"/images/9-run-multi-node-k8s-cluster-locally/cover.jpg","title":"Run multi-node kubernetes cluster locally within 15 minutes","readDuration":"15 min","categories":["K8s"]},"markdownBody":"\n## 1. Prepare tools\n\n- multipass 1.6.2: `brew install --cask multipass`\n- virtualbox 6.1.18: `brew install --cask virtualbox`\n\n> Note: You can install tools with other tools.\n\n## 2. Create 2 VMs\n\nCreate 2 nodes Ubuntu 20.04, 1G RAM, 1 CPU, 8GB disk.\n\n```bash\nmultipass launch -vvv -n node-1 -m 1G -d 8GB -c 1 20.04\nmultipass launch -vvv -n node-2 -m 1G -d 8GB -c 1 20.04\n```\n\nCheck in VirtualBox: `sudo VirtualBox`\n> Note: Open VirtualBox with sudo\n\n![Nodes in the cluster](/images/9-run-multi-node-k8s-cluster-locally/1-nodes-in-cluster.png)\n\n## 3. Configure the network\n\nBecause 2 VMs use network NAT, it has no a special ip. So we need to create 2 new networks: Host-only (nodes will communicate to each other with this network), NAT network (to allow nodes be able to access the internet) and disable the current NAT.\n\n\n3.1. In **VirtualBox**, open **Preferences**, create a new NatNetwork and enable DHCP\n\n![Create NAT network](/images/9-run-multi-node-k8s-cluster-locally/2-create-nat-network.png)\n\n![NAT network with DHCP](/images/9-run-multi-node-k8s-cluster-locally/3-nat-network-dhcp.png)\n\n3.2. Shutdown nodes\n\n![Shutdown nodes](/images/9-run-multi-node-k8s-cluster-locally/4-shutdown-nodes.png)\n\n3.3. Add new networks\n\n![Add new networks](/images/9-run-multi-node-k8s-cluster-locally/5-add-new-network-1.png)\n\n![Add new networks](/images/9-run-multi-node-k8s-cluster-locally/5-add-new-network-2.png)\n\n3.4. Start nodes\n\n![Start nodes](/images/9-run-multi-node-k8s-cluster-locally/6-start-nodes.png)\n\n3.5. If the `~/.ssh/id_rsa` file is not exist, generate a pair of ssh keys.\n\n```bash\nssh-keygen -C \"hieu@deptrai\"\ncat ~/.ssh/id_rsa.pub\n```\n\n3.6. SSH to node 1:\n\n```bash\nmultipass shell node-1\n```\n\n3.7. Write your ssh public key to node-1\n```bash\necho \"your_ssh_public_key\" | tee -a ~/.ssh/authorized_keys\n```\n\nReplace `your_ssh_public_key` with `~/.ssh/id_rsa.pub` file's content.\n\nExample:\n\n```bash\necho \"ssh-rsa AAAAB3NzaC1yc2E..... hieu@deptrai\" | tee -a ~/.ssh/authorized_keys\n```\n\n3.8. Enable 2 networks that we enabled\n\n```bash\nip link | grep DOWN\n```\n\nThey are usually `enp0s8` and `enp0s9`\n![Start nodes](/images/9-run-multi-node-k8s-cluster-locally/7-network-down.png)\n\nRun this command with `sudo`. Notice `enp0s8` and `enp0s9`.\n\n```bash\nsudo bash -c \"cat > /etc/netplan/60-bridge.yaml\" <<EOF\nnetwork:\n  ethernets:\n    enp0s8:\n      dhcp4: true\n      dhcp4-overrides:\n        route-metric: 200\n    enp0s9:\n      dhcp4: true\n      dhcp4-overrides:\n        route-metric: 200\n  version: 2\nEOF\n```\n```bash\nsudo netplan apply\nip addr show\n```\n\nThe `enp0s9`'s ip is `192.168.56.108`\n\n3.9. Exit ssh:\n```\nexit\n```\n\n3.10. Shutdown node-1 -> disable `NAT` at `Adapter 1` -> Start node-1\n\n![Disable network at adapter 1](/images/9-run-multi-node-k8s-cluster-locally/8-disable-network-adapter-1.png)\n\n3.11. Repeat steps `3.6`-`3.10` with `node-2`\n\nFinally, we have IPs:\n- Node-1: 192.168.56.108\n- Node-2: 192.168.56.109\n\n## 4. Setup a k8s cluster\n\n4.1. Install k3sup\n\n```bash\ncurl -sLS https://get.k3sup.dev | sh\nsudo install k3sup /usr/local/bin/\n```\n\n4.2. Install Kubernetes in `node-1` with `master` role.\n\n```bash\nk3sup install --ip 192.168.56.108 --user ubuntu --context k3s-1 --k3s-version v1.19.8+k3s1\n```\n\n4.3. Install Kubernetes in `node-2` with `worker` role.\n\n```bash\nk3sup join --ip 192.168.56.109 --server-ip 192.168.56.108 --user ubuntu --k3s-version v1.19.8+k3s1\n```\n\nIt will create `kubeconfig` file in your current directory.\n\n## 5. Test\n\n```bash\nexport KUBECONFIG=/Users/hieudeptrai/kubeconfig\nkubectl get node -o wide\n```\n\nDone!","slug":"9-run-multi-node-k8s-cluster-locally"},{"frontmatter":{"author":"Hieu Phan","date":"2020-10-07T08:00:00.000Z","image":"/images/8-toi-quan-ly-mat-khau-cua-minh-the-nao/cover.jpg","title":"Tôi quản lý 1 nùi mật khẩu thế nào?","readDuration":"15 min","categories":["Tip"]},"markdownBody":"\n## 1. Nhu cầu\n\n- Lưu được mật khẩu an toàn nhất có thể\n- Đồng bộ trên các nền tảng: Android, iOS, browser,..\n- Không giới hạn thiết bị\n- Hỗ trợ đăng nhập 2 lớp\n- Free càng tốt\n\n## 2. Tool mình đã từng sử dụng\n\n**Lastpass**\n\nTool khá xịn, đầy đủ các chức năng:\n\n- Đồng bộ trên các nền tảng: Android, iOS, browser,..\n- Không giới hạn thiết bị\n- Đặt biệt là free\n\n**1password**\n\nCó đầy đủ các tính năng xịn xò, nhưng không miễn phí.\n\n## 3. Giải pháp hiện tại mình đang dùng\n\nDạo 1 vòng tìm kiếm thì thấy những app có tính đăng nhập 2 lớp đều là gói premium hết.\n\nChuyển hướng qua tìm opensource thì thấy có thằng Bitwarden. App này có đủ tính năng mình cần và hỗ trợ self host.\n\n### Tìm 1 con server free:\n\n- EC2 t2.micro của Amazon\n- f1-micro của Google Cloud\n\n### Deploy Bitwarden server\n\n- Official (C#): https://github.com/bitwarden/server\n- Non-Official (Rust): https://github.com/dani-garcia/bitwarden_rs\n\nCó 2 option, ở đây mình dùng server Rust cho gọn nhẹ.\n\nChạy file `docker-compose.yaml`\n\n```yaml\nversion: '3'\n\nservices:\n  bitwarden:\n    image: bitwardenrs/server:1.16.3\n    restart: always\n    volumes:\n      - ./bw-data:/data\n    ports:\n      - 8080:80 # web, api, ...\n      - 3012:3012 # websockets\n    environment:\n      WEBSOCKET_ENABLED: 'true'\n      SIGNUPS_ALLOWED: 'true'\n      SHOW_PASSWORD_HINT: 'true'\n      ROCKET_WORKERS: 20\n      ADMIN_TOKEN: 'xxxxxxxhieudeptraixxxxxxx'\n      DOMAIN: https://xxxx.xxxx.xxx\n      SMTP_HOST: smtp.gmail.com\n      SMTP_FROM: abc@gmail.com\n      SMTP_PORT: 587\n      SMTP_SSL: 'true'\n      SMTP_USERNAME: abc@gmail.com\n      SMTP_PASSWORD: xxxxx\n```\n\nnginx config\n\n```nginx\nserver {\n    listen 80;\n    listen [::]:80;\n\n    server_name bws.example.com;\n\n    client_max_body_size 128M;\n\n    location / {\n        proxy_pass       http://localhost:8080;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n\n    location /notifications/hub {\n        proxy_pass         http://localhost:3012;\n        proxy_set_header   Host $host;\n        proxy_http_version 1.1;\n        proxy_set_header   Upgrade $http_upgrade;\n        proxy_set_header   Connection \"Upgrade\";\n    }\n\n    location /notifications/hub/negotiate {\n        proxy_pass http://localhost:8080;\n    }\n\n    location /admin {\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        proxy_pass http://localhost:8080;\n    }\n}\n```\n\nConfig thêm https cho an toàn hơn\n\n### Tạo user\n\nVào https://your-domain.com/admin, điền admin token trong file docker-compose.yaml vào.\n\n![Đăng ký/đăng nhập Bitwarden](/images/8-toi-quan-ly-mat-khau-cua-minh-the-nao/1-dang-nhap.png)\n\n### Cài extension/app\n\n- Browser extension: https://github.com/bitwarden/browser\n- Android: https://play.google.com/store/apps/details?id=com.x8bit.bitwarden\n- iOS: https://apps.apple.com/us/app/bitwarden-password-manager/id1137397744\n\n## 4. Một vài hình ảnh nhá hàng\n\nWeb\n\n![Web](/images/8-toi-quan-ly-mat-khau-cua-minh-the-nao/2-web.png)\n\nXem password\n\n![Xem password](/images/8-toi-quan-ly-mat-khau-cua-minh-the-nao/3-add-password.png)\n\nExtension\n\n![Extension](/images/8-toi-quan-ly-mat-khau-cua-minh-the-nao/4-extension.png)\n\nExtension nhận diện mật khẩu\n\n![Extension nhận diện mật khẩu](/images/8-toi-quan-ly-mat-khau-cua-minh-the-nao/5-extension-detect-password.png)","slug":"8-toi-quan-ly-mat-khau-cua-minh-the-nao"},{"frontmatter":{"author":"Hieu Phan","date":"2020-08-30T06:24:00.000Z","image":"/images/7-install-k3s-in-digitalocean/cover.jpg","title":"Setup K3s on Digitalocean","readDuration":"10 min","categories":["K8s"]},"markdownBody":"\n## 1. Create a Droplet\n\n![Create a Droplet](/images/7-install-k3s-in-digitalocean/1-create-a-droplet.png)\n\nResult:\n\n![Result Droplet](/images/7-install-k3s-in-digitalocean/2-result-droplet.jpg)\n\nTry to ssh to droplet.\n\n```bash\nssh root@123.456.789.012\n```\n\nInstall somethings:\n\n```bash\napt install curl\n```\n\nIf success, go to step 2.\n\n## 2. Install `k3sup` on your computer\n\n```bash\ncurl -sLS https://get.k3sup.dev | sh\n```\n\n## 3. Install k3s\n\n```bash\nk3sup install --ip 123.456.789.012 --user root\n```\n\n![Result 3-k3sup](/images/7-install-k3s-in-digitalocean/3-k3sup.png)\n\n## 4. Test\n\nAfter installing successfully, we have `kubeconfig` file.\n\n```bash\nexport KUBECONFIG=/Users/hieudeptrai/kubeconfig\nkubectl get node -o wide\n```\n\nDone!","slug":"7-install-k3s-in-digitalocean"},{"frontmatter":{"author":"Hieu Phan","date":"2020-06-22T15:22:00.000Z","image":"/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/cover.jpg","title":"Quản lý log tập trung trong microservice với ELK. Phần 1: trên docker swarm","readDuration":"30 min","categories":["Docker","Microservice"]},"markdownBody":"\n## 1. Kiến trúc\n\n![List node](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/architect.png)\n\n## 2. Khởi tạo docker swarm\n\nBạn có thể dùng nhiều server có cài sẵn docker để tạo thành 1 cluster. Tốt nhất là docker cùng version để tránh gặp những lỗi tào lao. Trong bài viết này mình dùng docker-machine để giả lập 3 node để demo.\n\n### 2.1. Tạo 1 cluster với 3 node\n\n```bash\ndocker-machine create -d virtualbox --virtualbox-memory \"2048\" node1\ndocker-machine create -d virtualbox --virtualbox-memory \"1024\" node2\ndocker-machine create -d virtualbox --virtualbox-memory \"1024\" node3\n```\n\nFixing `out of memory` error of Elasticsearch\n\n```bash\ndocker-machine ssh node1 sudo sysctl -w vm.max_map_count=262144\ndocker-machine ssh node2 sudo sysctl -w vm.max_map_count=262144\ndocker-machine ssh node3 sudo sysctl -w vm.max_map_count=262144\n```\n\nKiểm tra các node đã tạo\n\n```bash\ndocker-machine ls\n```\n\n![List node](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/1-list-node.png)\n\n### 2.2. Cho `node1` làm node manager.\n\n```bash\ndocker-machine ssh node1\ndocker swarm init --advertise-addr 192.168.99.104\n```\n\n`192.168.99.104` là ip của node 1, đã kiểm tra ở trên.\n\n![List node](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/2-init-swarm.png)\n\nKết quả ở trên cho ta 1 command để các node khác có thể join vào cluster.\n\n### 2.3. Join các node còn lại vào cluster\n\nTa lần lượt ssh vào từng node và chạy lệnh join ở trên.\n\n```bash\ndocker-machine ssh node2\ndocker swarm join --token SWMTKN-1-5dk4ysohsctxbifjmb6x8xa5i6qps81au5k4fmzzqmne3l3oz0-ac99bh5vrw8zm3lychgpu3cwh 192.168.99.104:2377\n```\n\n```bash\ndocker-machine ssh node3\ndocker swarm join --token SWMTKN-1-5dk4ysohsctxbifjmb6x8xa5i6qps81au5k4fmzzqmne3l3oz0-ac99bh5vrw8zm3lychgpu3cwh 192.168.99.104:2377\n```\n\n> Chú ý: `token` trong lệnh `docker swarm join` sẽ khác nhau ở mỗi lần chạy. Nên chỗ này đừng copy của mình nhé. Lỗi đấy!\n\n## 3. Deploy ELK lên docker swarm\n\nSSH vào node manager `docker-machine ssh node1`.\n\nTạo 1 file `docker-stack.yml` có nội dung như sau:\n\n```yaml\nversion: '3'\n\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.8.0\n    ports:\n      - \"9200:9200\"\n    volumes:\n      - esdata:/usr/share/elasticsearch/data\n    environment:\n      ES_JAVA_OPTS: \"-Xmx256m -Xms256m\"\n      ELASTIC_PASSWORD: \"changeme\"\n      discovery.type: single-node\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\n  logstash:\n    image: docker.elastic.co/logstash/logstash:7.8.0\n    volumes:\n      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf\n    depends_on:\n      - elasticsearch\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:7.8.0\n    environment:\n      ELASTICSEARCH_HOSTS: 'http://elasticsearch:9200'\n    ports:\n      - \"5601:5601\"\n    depends_on:\n      - logstash\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\n  logspout:\n    image: gliderlabs/logspout:v3\n    command: 'udp://logstash:5000'\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    depends_on:\n      - logstash\n    deploy:\n      mode: global\n      restart_policy:\n        condition: on-failure\n        delay: 30s\n\n  visualizer:\n    image: dockersamples/visualizer:stable\n    ports:\n      - \"8081:8080\"\n    volumes:\n      - \"/var/run/docker.sock:/var/run/docker.sock\"\n    labels:\n      app: \"visualizer\"\n    deploy:\n      placement:\n        constraints: [node.role == manager]\n\nvolumes:\n  esdata:\n    driver: local\n```\n\nTạo 1 file `logstash.conf` có nội dung như sau:\n\n```\ninput {\n   udp {\n   \t\tport => 5000\n   \t\tcodec => json\n   \t}\n}\n\n## Add your filters / logstash plugins configuration here\n\nfilter {\n  if [docker][image] =~ /logstash/ {\n    drop { }\n  }\n}\n\noutput {\n    elasticsearch {\n      hosts => \"elasticsearch:9200\"\n      user => \"elastic\"\n      password => \"changeme\"\n    }\n\n    stdout { codec => rubydebug }\n}\n```\n\nTiến hành deploy\n\n```bash\ndocker stack deploy -c docker-stack.yml elk\n```\n\nThường mình sẽ gặp 1 lỗi là logspout sẽ không kết nối được đến logstash, do logstash chưa init xong. Bạn hãy đợi 1 thời gian rồi chạy `docker service update --force elk_logspout` để restart lại logspout trên các node.\n\n## 4. Xem log từ Kibana\n\nDùng browser truy cập vào 1 node bất kỳ trong swarm với port `5601`\n\n![List node](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/1-list-node.png)\n\nTrong trường hợp của mình, mình sẽ truy cập vào `http://192.168.99.104:5601`.\n\n![Kibana](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/3-kibana-1.jpg)\n\nTạo 1 index pattern `logstash*`\n![Kibana](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/4-kibana-2.jpg)\n\n![Kibana](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/5-kibana-3.jpg)\n\n![Kibana](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/6-kibana-4.jpg)\n\n![Kibana](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/7-kibana-5.jpg)\n\n![Kibana](/images/6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm/8-kibana-6.png)\n\nOke! Tới đây giao diện để truy xuất log đã hiện ra. Bạn có thể filter log theo thời gian, theo app,... Chỗ này bạn có thể tự mò nhé, giao diện kibana cũng khá dễ sử dụng.","slug":"6-quan-ly-log-tap-trung-elk-phan-1-tren-docker-swarm"}],"title":"Hlog - Code for fun","description":"Cùng nhau chia sẻ kinh nghiệm, mò mẫm kiến thức mới","currentPage":"1","totalPage":2},"__N_SSG":true}